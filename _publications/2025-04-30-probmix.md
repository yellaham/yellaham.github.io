---
title: "Mixup Regularization: A Probabilistic Perspective"
collection: publications
permalink: /publication/2025-04-30-probmix
paperurl: 'https://arxiv.org/pdf/2502.13825'
date: 2025-04-30
venue: 'Uncertainty in Artificial Intelligence'
authors: '<b>Yousef El-Laham</b>, Niccolo Dalmasso, Svitlana Vyetrenko, Vamsi K. Potluru, Manuela Veloso'
pages: ""
year: 2025
publisher: 'PMLR'
---

<details>
<summary>Abstract</summary>
<br>
In recent years, mixup regularization has gained popularity as an effective way to improve the generalization 
performance of deep learning models by training on convex combinations of training data. While many mixup variants have 
been explored, the proper adoption of the technique to conditional density estimation and probabilistic machine learning 
remains relatively unexplored. This work introduces a novel framework for mixup regularization based on probabilistic 
fusion that is better suited for conditional density estimation tasks. For data distributed according to a member of 
the exponential family, we show that likelihood functions can be analytically fused using log-linear pooling. We 
further propose an extension of probabilistic mixup, which allows for fusion of inputs at an arbitrary intermediate 
layer of the neural network. We provide a theoretical analysis comparing our approach to standard mixup variants. 
Empirical results on synthetic and real datasets demonstrate the benefits of our proposed framework compared to existing 
mixup variants.
</details>